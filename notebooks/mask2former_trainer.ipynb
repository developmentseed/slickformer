{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c33df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn  # PyTorch Lightning NN (neural network) module\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "import torchdata\n",
    "import lightning as L\n",
    "from ceruleanml.data_pipeline import put_image_in_dict, get_src_pths_annotations\n",
    "from transformers import Mask2FormerForUniversalSegmentation, Mask2FormerConfig\n",
    "import os\n",
    "#for debugging\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "# Set the random seed\n",
    "seed=0 # we need to set this for torch datapipe separately\n",
    "random.seed(seed)\n",
    "torch.set_float32_matmul_precision('medium') # if you have tensor cores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524390ec",
   "metadata": {},
   "source": [
    "Loading the train and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4372685",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"/home/work/slickformer/data/partitions/train_tiles_context_0/\"\n",
    "train_imgs, train_annotations = get_src_pths_annotations(train_dir)\n",
    "val_dir = \"/home/work/slickformer/data/partitions/val_tiles_context_0/\"\n",
    "val_imgs, val_annotations = get_src_pths_annotations(val_dir)\n",
    "test_dir = \"/home/work/slickformer/data/partitions/test_tiles_context_0/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568c7059",
   "metadata": {},
   "source": [
    "Setting up the datapipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2254f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_i_coco_pipe = torchdata.datapipes.iter.IterableWrapper(iterable=[train_annotations])\n",
    "train_l_coco_pipe = torchdata.datapipes.iter.IterableWrapper(iterable=[train_annotations])\n",
    "train_source_pipe_processed = (\n",
    "    train_i_coco_pipe.get_scene_paths(train_dir)  # get source items from the collection\n",
    "    .read_tiff()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c0d42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_pipe_processed = (\n",
    "    train_l_coco_pipe.decode_masks()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838688ea",
   "metadata": {},
   "source": [
    "We'll train on random crops of masks to focus on the most informative parts of scene for more efficient training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4772818a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dp = (\n",
    "    train_source_pipe_processed.zip(train_labels_pipe_processed)\n",
    "    .random_crop_mask_if_exists(512,512)\n",
    "    .channel_first_norm_to_tensor()\n",
    "    .remap_remove()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee61e3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torchdata.datapipes.utils.to_graph(dp=train_dp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9b7382",
   "metadata": {},
   "source": [
    "Putting datapipes in a pytorch-lightning DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ed4166",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def collate_fn(batch):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in batch])\n",
    "    pixel_mask = torch.stack([example[\"pixel_mask\"] for example in batch])\n",
    "    class_labels = [example[\"class_labels\"] for example in batch]\n",
    "    mask_labels = [example[\"mask_labels\"] for example in batch]\n",
    "    return {\"pixel_values\": pixel_values, \"pixel_mask\": pixel_mask, \"class_labels\": class_labels, \"mask_labels\": mask_labels}\n",
    "\n",
    "\n",
    "class Mask2FormerDataModule(L.LightningDataModule):\n",
    "    def __init__(self, config_path, train_dir, val_dir, test_dir, batch_size, num_workers, crop_size=512):\n",
    "        super().__init__()\n",
    "        self.train_dir, self.val_dir, self.test_dir = train_dir, val_dir, test_dir\n",
    "        self.bs = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.crop_size = crop_size\n",
    "        self.config_path = config_path\n",
    "\n",
    "    def setup(self, stage):\n",
    "        if stage is not None:  # train/val/test/predict\n",
    "            train_imgs, train_annotations = get_src_pths_annotations(self.train_dir)\n",
    "            train_i_coco_pipe = torchdata.datapipes.iter.IterableWrapper(iterable=[train_annotations])\n",
    "            train_l_coco_pipe = torchdata.datapipes.iter.IterableWrapper(iterable=[train_annotations])\n",
    "            train_source_pipe_processed = (\n",
    "                train_i_coco_pipe.get_scene_paths(self.train_dir)  # get source items from the collection\n",
    "                .read_tiff()\n",
    "            )\n",
    "            train_labels_pipe_processed = (\n",
    "                train_l_coco_pipe.decode_masks()\n",
    "            )\n",
    "            self.train_dp = (\n",
    "                train_source_pipe_processed.zip(train_labels_pipe_processed)\n",
    "                .random_crop_mask_if_exists(self.crop_size, self.crop_size)\n",
    "                .channel_first_norm_to_tensor()\n",
    "                .remap_remove()\n",
    "            )\n",
    "            # TODO if val processing mirrors train processing, this could be factored out to a func\n",
    "            val_imgs, val_annotations = get_src_pths_annotations(self.val_dir)\n",
    "            val_i_coco_pipe = torchdata.datapipes.iter.IterableWrapper(iterable=[val_annotations])\n",
    "            val_l_coco_pipe = torchdata.datapipes.iter.IterableWrapper(iterable=[val_annotations])\n",
    "            val_source_pipe_processed = (\n",
    "                val_i_coco_pipe.get_scene_paths(self.val_dir) # get source items from the collection\n",
    "                .  read_tiff()\n",
    "            )\n",
    "            val_labels_pipe_processed = (\n",
    "                val_l_coco_pipe.decode_masks()\n",
    "            )\n",
    "            self.val_dp = (\n",
    "                val_source_pipe_processed.zip(val_labels_pipe_processed)\n",
    "                .random_crop_mask_if_exists(self.crop_size,self.crop_size)\n",
    "                .channel_first_norm_to_tensor()\n",
    "                .remap_remove()\n",
    "            )\n",
    "\n",
    "            test_imgs, test_annotations = get_src_pths_annotations(self.test_dir)\n",
    "            test_i_coco_pipe = torchdata.datapipes.iter.IterableWrapper(iterable=[test_annotations])\n",
    "            test_l_coco_pipe = torchdata.datapipes.iter.IterableWrapper(iterable=[test_annotations])\n",
    "            test_source_pipe_processed = (\n",
    "            test_i_coco_pipe.get_scene_paths(self.test_dir) # get source items from the collection\n",
    "                .read_tiff()\n",
    "                .map(put_image_in_dict)\n",
    "            )\n",
    "            test_labels_pipe_processed = (\n",
    "                test_l_coco_pipe.decode_masks()\n",
    "            )\n",
    "            self.test_dp = (\n",
    "            test_source_pipe_processed.zip(test_labels_pipe_processed)\n",
    "            .combine_src_label_dicts() # we don't crop for the test set TODO, do we also not crop for validation?\n",
    "            .channel_first_norm_to_tensor()\n",
    "            .remap_remove()\n",
    "            )\n",
    "\n",
    "    def graph_dp(self):\n",
    "        return torchdata.datapipes.utils.to_graph(dp=self.train_dp)\n",
    "\n",
    "    def show_batch(self, channel=0):\n",
    "        \"\"\"\n",
    "        channel 0 - vv radar\n",
    "        channel 1 infra distance\n",
    "        channel 2 historical vessel traffic\n",
    "\n",
    "        \"\"\"\n",
    "        assert channel in [0,1,2]\n",
    "\n",
    "        def closest_factors(n):\n",
    "            factor1 = int(n**0.5)\n",
    "            factor2 = n // factor1\n",
    "            while factor1 * factor2 != n:\n",
    "                factor1 -= 1\n",
    "                factor2 = n // factor1\n",
    "            return factor1, factor2\n",
    "\n",
    "        nrows, ncols = closest_factors(self.bs)\n",
    "        fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(10, 10))\n",
    "        \n",
    "        for item, ax in zip(self.train_dp, fig.axes):\n",
    "            if channel == 0:\n",
    "                vv_db_units = np.log10(np.array(item[\"image\"][channel,:,:])) * 10\n",
    "            else:\n",
    "                vv_db_units = np.array(item[\"image\"][channel,:,:])\n",
    "            vv_db_units[vv_db_units == -np.inf] = np.nan\n",
    "            min_value = np.nanpercentile(vv_db_units, 5)\n",
    "            max_value = np.nanpercentile(vv_db_units, 95)\n",
    "            rescaled = (vv_db_units - min_value) / (max_value - min_value)\n",
    "            im = ax.imshow(rescaled)\n",
    "            \n",
    "            # Create an individual colorbar for the current image\n",
    "            cbar = fig.colorbar(im, ax=ax, shrink=0.7)\n",
    "            \n",
    "            # Set the number of ticks on the colorbar\n",
    "            cbar.locator = MaxNLocator(nbins=5)\n",
    "            cbar.update_ticks()\n",
    "\n",
    "            # Format the tick labels\n",
    "            tick_formatter = FuncFormatter(lambda x, pos: f'{min_value + x * (max_value - min_value):.2f}')\n",
    "            cbar.ax.yaxis.set_major_formatter(tick_formatter)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(num_workers= self.num_workers, pin_memory=True, dataset=self.train_dp.remap_remove().m2fprocessor(self.config_path).batch(self.bs).map(collate_fn), batch_size=None)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(num_workers= self.num_workers, pin_memory=True, dataset=self.val_dp.remap_remove().m2fprocessor(self.config_path).batch(self.bs).map(collate_fn), batch_size=None)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(num_workers= self.num_workers, pin_memory=True, dataset=self.test_dp.remap_remove().m2fprocessor(self.config_path).batch(self.bs).map(collate_fn), batch_size=None)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(num_workers= self.num_workers, pin_memory=True, dataset=self.test_dp.remap_remove().m2fprocessor(self.config_path).batch(self.bs).map(collate_fn), batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff489118",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "data_config_path= \"/home/work/slickformer/custom_processors/preprocessor_config.json\"\n",
    "onef_dm = Mask2FormerDataModule(data_config_path, train_dir, val_dir, test_dir, batch_size=10, num_workers=os.cpu_count() - 1, crop_size=512)\n",
    "# 10 i slimit for 24 Gb gpu memory and 512 crop size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caed843",
   "metadata": {},
   "outputs": [],
   "source": [
    "onef_dm.setup(stage=\"train\") #what's the purpose of stage?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2782ea53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MaxNLocator, FuncFormatter\n",
    "\n",
    "onef_dm.show_batch(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2f5021",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in onef_dm.train_dataloader():\n",
    "    i\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e61ebc4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "class Backbone(nn.Module):\n",
    "    def __init__(\n",
    "        self, model_name, in_chans, num_classes, pretrained, global_pool, drop_rate\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # loads from huggingface if not downloaded\n",
    "        \n",
    "        #by default the above method sets eval mode, set to training\n",
    "        self.backbone.train()\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return self.backbone(xb)\n",
    "\n",
    "class Mask2FormerLightningModel(L.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_path,\n",
    "        learning_rate: float = 2e-5,\n",
    "        adam_epsilon: float = 1e-8,\n",
    "        warmup_steps: int = 0,\n",
    "        weight_decay: float = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()  # saves all hparams as self.hparams\n",
    "        # can try other universal segmentation models: https://github.com/huggingface/transformers/pull/20766/files#r1050493186\n",
    "        config = Mask2FormerConfig.from_pretrained(config_path, ignore_mismatched_sizes=True) \n",
    "        self.model = Mask2FormerForUniversalSegmentation.from_pretrained(\"facebook/mask2former-swin-small-cityscapes-panoptic\", config=config, cache_dir=\"./model_cache\", ignore_mismatched_sizes=True)\n",
    "        #by default the above method sets eval mode, set to training\n",
    "        self.model.train()\n",
    "        # Move the model to the default CUDA device (if available)\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(device)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return self.model(xb)\n",
    "\n",
    "    def one_step(self, batch):\n",
    "        # potential edge case, we squash instance masks to semantic masks. it's possible to lose \n",
    "        # mask label 3 from mask but not class labels since we don't edit class labels\n",
    "        #self.model.train() # for some reason this needs to be set here, not picked up in init\n",
    "        outputs = self.model(pixel_values=batch[\"pixel_values\"], mask_labels=batch[\"mask_labels\"], class_labels=batch['class_labels'])\n",
    "        return outputs.loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Prepare optimizer and schedule (linear warmup and decay)\"\"\"\n",
    "        optimizer = AdamW(self.model.parameters(), lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n",
    "        return [optimizer]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self.one_step(batch)\n",
    "        self.log(\"trn_loss\", loss, on_step=True, on_epoch=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self.one_step(batch)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, score = self.one_step(batch)\n",
    "        self.log(\"tst_loss\", loss, prog_bar=True, logger=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f3f987",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.profilers import AdvancedProfile, SimpleProfiler\n",
    "\n",
    "profiler = SimpleProfiler(dirpath=\".\", filename=\"perf_logs\")\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs = 5,\n",
    "    accelerator=\"auto\",\n",
    "    devices = 1 if torch.cuda.is_available else None,\n",
    "    logger=logger,\n",
    "    profiler=profiler\n",
    "    # deterministic=True # can't set this when using cuda\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c3d5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config_path= \"/home/work/slickformer/custom_models/config.json\"\n",
    "model = Mask2FormerLightningModel(model_config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d9e2d2",
   "metadata": {},
   "source": [
    "need to run this in terminal because for some reason created dirs are owned by root even though docker container built for user 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002d4faa",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "trainer.fit(model, datamodule=onef_dm)\n",
    "print(profiler.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aaad40a",
   "metadata": {
    "lines_to_next_cell": 3
   },
   "outputs": [],
   "source": [
    "# from transformers import  AutoImageProcessor, MaskFormerForInstanceSegmentation\n",
    "# image_processor = AutoImageProcessor.from_pretrained(\"facebook/maskformer-swin-base-coco\", num_labels=3)\n",
    "# instance_inputs = image_processor(images=data['image'], return_tensors=\"pt\")\n",
    "# for k,v in instance_inputs.items():\n",
    "#   print(k,v.shape)\n",
    "\n",
    "\n",
    "# from collections import defaultdict\n",
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib import cm\n",
    "# import matplotlib.patches as mpatches\n",
    "\n",
    "# def draw_panoptic_segmentation(segmentation, segments_info):\n",
    "#     # get the used color map\n",
    "#     viridis = cm.get_cmap('viridis', torch.max(segmentation))\n",
    "#     fig, ax = plt.subplots()\n",
    "#     ax.imshow(segmentation)\n",
    "#     instances_counter = defaultdict(int)\n",
    "#     handles = []\n",
    "#     # for each segment, draw its legend\n",
    "#     for segment in segments_info:\n",
    "#         segment_id = segment['id']\n",
    "#         segment_label_id = segment['label_id']\n",
    "#         segment_label = model.config.id2label[segment_label_id]\n",
    "#         label = f\"{segment_label}-{instances_counter[segment_label_id]}\"\n",
    "#         instances_counter[segment_label_id] += 1\n",
    "#         color = viridis(segment_id)\n",
    "#         handles.append(mpatches.Patch(color=color, label=label))\n",
    "        \n",
    "#     ax.legend(handles=handles)\n",
    "#     plt.savefig('cats_panoptic.png')\n",
    "\n",
    "# draw_panoptic_segmentation(**panoptic_segmentation)\n",
    "\n",
    "# # %%\n",
    "\n",
    "\n",
    "# # %%\n",
    "\n",
    "\n",
    "# # %%\n",
    "\n",
    "\n",
    "# # %%\n",
    "\n",
    "\n",
    "# # %%\n",
    "\n",
    "\n",
    "# # %% [markdown]\n",
    "# # Groundtruth datapipe with non cropped images. We will use these for inference with the trained model.\n",
    "\n",
    "# # %%\n",
    "# gt_train_dp = (train_dp\n",
    "#                     .map(evaluation.remap_gt_dict)\n",
    "#                     .map(evaluation.stack_boxes)\n",
    "# )\n",
    "\n",
    "# # %%\n",
    "# from torchmetrics import detection\n",
    "\n",
    "# m = detection.mean_ap.MeanAveragePrecision(box_format='xyxy', iou_type='bbox', iou_thresholds=[.5], rec_thresholds=None, max_detection_thresholds=None, class_metrics=True)\n",
    "\n",
    "# m.update(preds=[pred_dict_thresholded_nms], target=[test_sample])\n",
    "\n",
    "# from pprint import pprint\n",
    "# pprint(m.compute())"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
