{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45c33df0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m  \u001b[38;5;66;03m# PyTorch Lightning NN (neural network) module\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn  # PyTorch Lightning NN (neural network) module\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "import torchdata\n",
    "import lightning as L\n",
    "from ceruleanml.data_pipeline import put_image_in_dict, get_src_pths_annotations\n",
    "from transformers import Mask2FormerForUniversalSegmentation, Mask2FormerConfig\n",
    "import os\n",
    "#for debugging\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "# Set the random seed\n",
    "seed=0 # we need to set this for torch datapipe separately\n",
    "random.seed(seed)\n",
    "torch.set_float32_matmul_precision('medium') # if you have tensor cores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "524390ec",
   "metadata": {},
   "source": [
    "Loading the train and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4372685",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"/home/work/slickformer/data/partitions/train_tiles_context_0/\"\n",
    "train_imgs, train_annotations = get_src_pths_annotations(train_dir)\n",
    "val_dir = \"/home/work/slickformer/data/partitions/val_tiles_context_0/\"\n",
    "val_imgs, val_annotations = get_src_pths_annotations(val_dir)\n",
    "test_dir = \"/home/work/slickformer/data/partitions/test_tiles_context_0/\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "568c7059",
   "metadata": {},
   "source": [
    "Setting up the datapipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2254f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_i_coco_pipe = torchdata.datapipes.iter.IterableWrapper(iterable=[train_annotations])\n",
    "train_l_coco_pipe = torchdata.datapipes.iter.IterableWrapper(iterable=[train_annotations])\n",
    "train_source_pipe_processed = (\n",
    "    train_i_coco_pipe.get_scene_paths(train_dir)  # get source items from the collection\n",
    "    .read_tiff()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77c0d42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_pipe_processed = (\n",
    "    train_l_coco_pipe.decode_masks()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfe019c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 547 ms, sys: 31.3 ms, total: 578 ms\n",
      "Wall time: 595 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'masks': [array([[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)],\n",
       " 'labels': [3],\n",
       " 'image_name': 'S1B_IW_GRDH_1SDV_20210723T164019_20210723T164044_027924_0354FB_5BDF.tif'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "next(iter(train_labels_pipe_processed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49ed8fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_pipe_processed = (\n",
    "    train_l_coco_pipe.read_masks(\"/home/work/slickformer/data/partitions/train_tiles_context_0/tiled_masks\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00370336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 505 ms, sys: 15.2 ms, total: 520 ms\n",
      "Wall time: 537 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'masks': [array([[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)],\n",
       " 'labels': [3],\n",
       " 'image_name': 'S1B_IW_GRDH_1SDV_20210723T164019_20210723T164044_027924_0354FB_5BDF.tif'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "next(iter(train_labels_pipe_processed))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "838688ea",
   "metadata": {},
   "source": [
    "We'll train on random crops of masks to focus on the most informative parts of scene for more efficient training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4772818a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dp = (\n",
    "    train_source_pipe_processed.zip(train_labels_pipe_processed)\n",
    "    .random_crop_mask_if_exists(512,512)\n",
    "    .channel_first_norm_to_tensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee61e3e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 7.1.0 (20230122.1345)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"234pt\" height=\"302pt\"\n",
       " viewBox=\"0.00 0.00 234.00 302.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 298)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-298 230,-298 230,4 -4,4\"/>\n",
       "<!-- IterableWrapper&#45;8794688025028 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>IterableWrapper&#45;8794688025028</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"107,-294 0,-294 0,-275 107,-275 107,-294\"/>\n",
       "<text text-anchor=\"middle\" x=\"53.5\" y=\"-282\" font-family=\"monospace\" font-size=\"10.00\">IterableWrapper</text>\n",
       "</g>\n",
       "<!-- GetScenePaths&#45;8794688024998 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>GetScenePaths&#45;8794688024998</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"101,-239 6,-239 6,-220 101,-220 101,-239\"/>\n",
       "<text text-anchor=\"middle\" x=\"53.5\" y=\"-227\" font-family=\"monospace\" font-size=\"10.00\">GetScenePaths</text>\n",
       "</g>\n",
       "<!-- IterableWrapper&#45;8794688025028&#45;&gt;GetScenePaths&#45;8794688024998 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>IterableWrapper&#45;8794688025028&#45;&gt;GetScenePaths&#45;8794688024998</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M53.5,-274.75C53.5,-268.27 53.5,-259.16 53.5,-250.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"57,-250.96 53.5,-240.96 50,-250.96 57,-250.96\"/>\n",
       "</g>\n",
       "<!-- ReadTiff&#45;8794388148601 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>ReadTiff&#45;8794388148601</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"94,-184 29,-184 29,-165 94,-165 94,-184\"/>\n",
       "<text text-anchor=\"middle\" x=\"61.5\" y=\"-172\" font-family=\"monospace\" font-size=\"10.00\">ReadTiff</text>\n",
       "</g>\n",
       "<!-- GetScenePaths&#45;8794688024998&#45;&gt;ReadTiff&#45;8794388148601 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>GetScenePaths&#45;8794688024998&#45;&gt;ReadTiff&#45;8794388148601</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M54.82,-219.75C55.81,-213.19 57.21,-203.95 58.47,-195.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"61.89,-196.35 59.92,-185.94 54.97,-195.3 61.89,-196.35\"/>\n",
       "</g>\n",
       "<!-- Zipper&#45;8794688025034 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>Zipper&#45;8794688025034</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"139.5,-129 85.5,-129 85.5,-110 139.5,-110 139.5,-129\"/>\n",
       "<text text-anchor=\"middle\" x=\"112.5\" y=\"-117\" font-family=\"monospace\" font-size=\"10.00\">Zipper</text>\n",
       "</g>\n",
       "<!-- ReadTiff&#45;8794388148601&#45;&gt;Zipper&#45;8794688025034 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>ReadTiff&#45;8794388148601&#45;&gt;Zipper&#45;8794688025034</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M69.92,-164.75C76.97,-157.43 87.23,-146.76 95.91,-137.74\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"98.43,-140.17 102.85,-130.53 93.39,-135.31 98.43,-140.17\"/>\n",
       "</g>\n",
       "<!-- ReadMasks&#45;8794688024857 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>ReadMasks&#45;8794688024857</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"199,-184 128,-184 128,-165 199,-165 199,-184\"/>\n",
       "<text text-anchor=\"middle\" x=\"163.5\" y=\"-172\" font-family=\"monospace\" font-size=\"10.00\">ReadMasks</text>\n",
       "</g>\n",
       "<!-- ReadMasks&#45;8794688024857&#45;&gt;Zipper&#45;8794688025034 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>ReadMasks&#45;8794688024857&#45;&gt;Zipper&#45;8794688025034</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M155.08,-164.75C148.03,-157.43 137.77,-146.76 129.09,-137.74\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"131.61,-135.31 122.15,-130.53 126.57,-140.17 131.61,-135.31\"/>\n",
       "</g>\n",
       "<!-- RandomCropByMasks&#45;8794688024854 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>RandomCropByMasks&#45;8794688024854</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"172,-74 53,-74 53,-55 172,-55 172,-74\"/>\n",
       "<text text-anchor=\"middle\" x=\"112.5\" y=\"-62\" font-family=\"monospace\" font-size=\"10.00\">RandomCropByMasks</text>\n",
       "</g>\n",
       "<!-- Zipper&#45;8794688025034&#45;&gt;RandomCropByMasks&#45;8794688024854 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>Zipper&#45;8794688025034&#45;&gt;RandomCropByMasks&#45;8794688024854</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M112.5,-109.75C112.5,-103.27 112.5,-94.16 112.5,-85.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"116,-85.96 112.5,-75.96 109,-85.96 116,-85.96\"/>\n",
       "</g>\n",
       "<!-- ChannelFirstNormToTensor&#45;8794355189547 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>ChannelFirstNormToTensor&#45;8794355189547</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"193,-19 32,-19 32,0 193,0 193,-19\"/>\n",
       "<text text-anchor=\"middle\" x=\"112.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\">ChannelFirstNormToTensor</text>\n",
       "</g>\n",
       "<!-- RandomCropByMasks&#45;8794688024854&#45;&gt;ChannelFirstNormToTensor&#45;8794355189547 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>RandomCropByMasks&#45;8794688024854&#45;&gt;ChannelFirstNormToTensor&#45;8794355189547</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M112.5,-54.75C112.5,-48.27 112.5,-39.16 112.5,-30.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"116,-30.96 112.5,-20.96 109,-30.96 116,-30.96\"/>\n",
       "</g>\n",
       "<!-- IterableWrapper&#45;8794688025010 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>IterableWrapper&#45;8794688025010</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"226,-239 119,-239 119,-220 226,-220 226,-239\"/>\n",
       "<text text-anchor=\"middle\" x=\"172.5\" y=\"-227\" font-family=\"monospace\" font-size=\"10.00\">IterableWrapper</text>\n",
       "</g>\n",
       "<!-- IterableWrapper&#45;8794688025010&#45;&gt;ReadMasks&#45;8794688024857 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>IterableWrapper&#45;8794688025010&#45;&gt;ReadMasks&#45;8794688024857</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M171.01,-219.75C169.9,-213.19 168.33,-203.95 166.91,-195.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"170.4,-195.21 165.27,-185.93 163.5,-196.38 170.4,-195.21\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7ffac4171fa0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchdata.datapipes.utils.to_graph(dp=train_dp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d9b7382",
   "metadata": {},
   "source": [
    "Putting datapipes in a pytorch-lightning DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71ed4166",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def collate_fn(batch):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in batch])\n",
    "    pixel_mask = torch.stack([example[\"pixel_mask\"] for example in batch])\n",
    "    class_labels = [example[\"class_labels\"] for example in batch]\n",
    "    mask_labels = [example[\"mask_labels\"] for example in batch]\n",
    "    return {\"pixel_values\": pixel_values, \"pixel_mask\": pixel_mask, \"class_labels\": class_labels, \"mask_labels\": mask_labels}\n",
    "\n",
    "\n",
    "class Mask2FormerDataModule(L.LightningDataModule):\n",
    "    def __init__(self, config_path, train_dir, val_dir, test_dir, batch_size, num_workers, crop_size=512):\n",
    "        super().__init__()\n",
    "        self.train_dir, self.val_dir, self.test_dir = train_dir, val_dir, test_dir\n",
    "        self.bs = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.crop_size = crop_size\n",
    "        self.config_path = config_path\n",
    "\n",
    "    def setup(self, stage):\n",
    "        if stage is not None:  # train/val/test/predict\n",
    "            train_imgs, train_annotations = get_src_pths_annotations(self.train_dir)\n",
    "            train_i_coco_pipe = torchdata.datapipes.iter.IterableWrapper(iterable=[train_annotations])\n",
    "            train_l_coco_pipe = torchdata.datapipes.iter.IterableWrapper(iterable=[train_annotations])\n",
    "            train_source_pipe_processed = (\n",
    "                train_i_coco_pipe.get_scene_paths(self.train_dir)  # get source items from the collection\n",
    "                .read_tiff()\n",
    "            )\n",
    "            train_labels_pipe_processed = (\n",
    "                train_l_coco_pipe.decode_masks()\n",
    "            )\n",
    "            self.train_dp = (\n",
    "                train_source_pipe_processed.zip(train_labels_pipe_processed)\n",
    "                .random_crop_mask_if_exists(self.crop_size, self.crop_size)\n",
    "                .channel_first_norm_to_tensor()\n",
    "                .remap_remove()\n",
    "            )\n",
    "            # TODO if val processing mirrors train processing, this could be factored out to a func\n",
    "            val_imgs, val_annotations = get_src_pths_annotations(self.val_dir)\n",
    "            val_i_coco_pipe = torchdata.datapipes.iter.IterableWrapper(iterable=[val_annotations])\n",
    "            val_l_coco_pipe = torchdata.datapipes.iter.IterableWrapper(iterable=[val_annotations])\n",
    "            val_source_pipe_processed = (\n",
    "                val_i_coco_pipe.get_scene_paths(self.val_dir) # get source items from the collection\n",
    "                .  read_tiff()\n",
    "            )\n",
    "            val_labels_pipe_processed = (\n",
    "                val_l_coco_pipe.decode_masks()\n",
    "            )\n",
    "            self.val_dp = (\n",
    "                val_source_pipe_processed.zip(val_labels_pipe_processed)\n",
    "                .random_crop_mask_if_exists(self.crop_size,self.crop_size)\n",
    "                .channel_first_norm_to_tensor()\n",
    "                .remap_remove()\n",
    "            )\n",
    "\n",
    "            test_imgs, test_annotations = get_src_pths_annotations(self.test_dir)\n",
    "            test_i_coco_pipe = torchdata.datapipes.iter.IterableWrapper(iterable=[test_annotations])\n",
    "            test_l_coco_pipe = torchdata.datapipes.iter.IterableWrapper(iterable=[test_annotations])\n",
    "            test_source_pipe_processed = (\n",
    "            test_i_coco_pipe.get_scene_paths(self.test_dir) # get source items from the collection\n",
    "                .read_tiff()\n",
    "                .map(put_image_in_dict)\n",
    "            )\n",
    "            test_labels_pipe_processed = (\n",
    "                test_l_coco_pipe.decode_masks()\n",
    "            )\n",
    "            self.test_dp = (\n",
    "            test_source_pipe_processed.zip(test_labels_pipe_processed)\n",
    "            .combine_src_label_dicts() # we don't crop for the test set TODO, do we also not crop for validation?\n",
    "            .channel_first_norm_to_tensor()\n",
    "            .remap_remove()\n",
    "            )\n",
    "\n",
    "    def graph_dp(self):\n",
    "        return torchdata.datapipes.utils.to_graph(dp=self.train_dp)\n",
    "\n",
    "    def show_batch(self, channel=0):\n",
    "        \"\"\"\n",
    "        channel 0 - vv radar\n",
    "        channel 1 infra distance\n",
    "        channel 2 historical vessel traffic\n",
    "\n",
    "        \"\"\"\n",
    "        assert channel in [0,1,2]\n",
    "\n",
    "        def closest_factors(n):\n",
    "            factor1 = int(n**0.5)\n",
    "            factor2 = n // factor1\n",
    "            while factor1 * factor2 != n:\n",
    "                factor1 -= 1\n",
    "                factor2 = n // factor1\n",
    "            return factor1, factor2\n",
    "\n",
    "        nrows, ncols = closest_factors(self.bs)\n",
    "        fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(10, 10))\n",
    "        \n",
    "        for item, ax in zip(self.train_dp, fig.axes):\n",
    "            if channel == 0:\n",
    "                vv_db_units = np.log10(np.array(item[\"image\"][channel,:,:])) * 10\n",
    "            else:\n",
    "                vv_db_units = np.array(item[\"image\"][channel,:,:])\n",
    "            vv_db_units[vv_db_units == -np.inf] = np.nan\n",
    "            min_value = np.nanpercentile(vv_db_units, 5)\n",
    "            max_value = np.nanpercentile(vv_db_units, 95)\n",
    "            rescaled = (vv_db_units - min_value) / (max_value - min_value)\n",
    "            im = ax.imshow(rescaled)\n",
    "            \n",
    "            # Create an individual colorbar for the current image\n",
    "            cbar = fig.colorbar(im, ax=ax, shrink=0.7)\n",
    "            \n",
    "            # Set the number of ticks on the colorbar\n",
    "            cbar.locator = MaxNLocator(nbins=5)\n",
    "            cbar.update_ticks()\n",
    "\n",
    "            # Format the tick labels\n",
    "            tick_formatter = FuncFormatter(lambda x, pos: f'{min_value + x * (max_value - min_value):.2f}')\n",
    "            cbar.ax.yaxis.set_major_formatter(tick_formatter)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_dp_batched, _ = self.train_dp.m2fprocessor(self.config_path).batch(self.bs).map(collate_fn).random_split(total_length=10, seed =0)\n",
    "        return DataLoader(num_workers= self.num_workers, pin_memory=True, dataset=train_dp_batched, batch_size=None)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_dp_batched = self.val_dp.m2fprocessor(self.config_path).batch(self.bs).map(collate_fn)\n",
    "        return DataLoader(num_workers= self.num_workers, pin_memory=True, dataset=val_dp_batched, batch_size=None)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        test_dp_batched = self.test_dp.m2fprocessor(self.config_path).batch(self.bs).map(collate_fn)\n",
    "        return DataLoader(num_workers= self.num_workers, pin_memory=True, dataset=test_dp_batched, batch_size=None)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        test_dp_batched = self.test_dp.m2fprocessor(self.config_path).batch(self.bs).map(collate_fn)\n",
    "        return DataLoader(num_workers= self.num_workers, pin_memory=True, dataset=test_dp_batched, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff489118",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "data_config_path= \"/home/work/slickformer/custom_processors/preprocessor_config.json\"\n",
    "onef_dm = Mask2FormerDataModule(data_config_path, train_dir, val_dir, test_dir, batch_size=10, num_workers=os.cpu_count() - 1, crop_size=512)\n",
    "# 10 is limit for 24 Gb gpu memory and 512 crop size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a175cbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dp_batched, _ = train_dp.m2fprocessor(data_config_path).batch(10).map(collate_fn).random_split(total_length=10, weights = {\"train\":1, \"valid\":0},seed =0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19f67a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 770 ms, sys: 183 ms, total: 953 ms\n",
      "Wall time: 840 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "x = next(iter(train_dp.remap_remove().m2fprocessor(data_config_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6706855f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.15 s, sys: 61.3 ms, total: 1.21 s\n",
      "Wall time: 701 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "x=next(iter(train_dp.remap_remove()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bc8d7e31",
   "metadata": {},
   "source": [
    "for some reason this errors after batch.... might want to go back to long iteration over all batches commit.  doe sthis happen with random split?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3dc46d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "zeros_like(): argument 'input' (position 1) must be Tensor, not numpy.ndarray\nThis exception is thrown by __iter__ of Mask2FormerProcessorDP(kwargs={}, sample_dicts=ChannelFirstNormToTensor)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m\n",
      "File \u001b[0;32m~/mambaforge/envs/slickformer/lib/python3.9/site-packages/torch/utils/data/datapipes/_hook_iterator.py:173\u001b[0m, in \u001b[0;36mhook_iterator.<locals>.wrap_generator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    171\u001b[0m         response \u001b[39m=\u001b[39m gen\u001b[39m.\u001b[39msend(\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    172\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 173\u001b[0m     response \u001b[39m=\u001b[39m gen\u001b[39m.\u001b[39;49msend(\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    175\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    176\u001b[0m     datapipe\u001b[39m.\u001b[39m_number_of_samples_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/mambaforge/envs/slickformer/lib/python3.9/site-packages/torchdata/datapipes/iter/util/randomsplitter.py:184\u001b[0m, in \u001b[0;36mSplitterIterator.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__iter__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    183\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmain_datapipe\u001b[39m.\u001b[39mreset()\n\u001b[0;32m--> 184\u001b[0m     \u001b[39mfor\u001b[39;00m sample \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmain_datapipe\u001b[39m.\u001b[39msource_datapipe:\n\u001b[1;32m    185\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmain_datapipe\u001b[39m.\u001b[39mdraw() \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget:\n\u001b[1;32m    186\u001b[0m             \u001b[39myield\u001b[39;00m sample\n",
      "File \u001b[0;32m~/mambaforge/envs/slickformer/lib/python3.9/site-packages/torch/utils/data/datapipes/_hook_iterator.py:173\u001b[0m, in \u001b[0;36mhook_iterator.<locals>.wrap_generator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    171\u001b[0m         response \u001b[39m=\u001b[39m gen\u001b[39m.\u001b[39msend(\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    172\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 173\u001b[0m     response \u001b[39m=\u001b[39m gen\u001b[39m.\u001b[39;49msend(\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    175\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    176\u001b[0m     datapipe\u001b[39m.\u001b[39m_number_of_samples_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/mambaforge/envs/slickformer/lib/python3.9/site-packages/torch/utils/data/datapipes/iter/callable.py:122\u001b[0m, in \u001b[0;36mMapperIterDataPipe.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__iter__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator[T_co]:\n\u001b[0;32m--> 122\u001b[0m     \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdatapipe:\n\u001b[1;32m    123\u001b[0m         \u001b[39myield\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply_fn(data)\n",
      "File \u001b[0;32m~/mambaforge/envs/slickformer/lib/python3.9/site-packages/torch/utils/data/datapipes/_hook_iterator.py:173\u001b[0m, in \u001b[0;36mhook_iterator.<locals>.wrap_generator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    171\u001b[0m         response \u001b[39m=\u001b[39m gen\u001b[39m.\u001b[39msend(\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    172\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 173\u001b[0m     response \u001b[39m=\u001b[39m gen\u001b[39m.\u001b[39;49msend(\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    175\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    176\u001b[0m     datapipe\u001b[39m.\u001b[39m_number_of_samples_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/mambaforge/envs/slickformer/lib/python3.9/site-packages/torch/utils/data/datapipes/iter/grouping.py:70\u001b[0m, in \u001b[0;36mBatcherIterDataPipe.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__iter__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator[DataChunk]:\n\u001b[1;32m     69\u001b[0m     batch: List \u001b[39m=\u001b[39m []\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdatapipe:\n\u001b[1;32m     71\u001b[0m         batch\u001b[39m.\u001b[39mappend(x)\n\u001b[1;32m     72\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(batch) \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size:\n",
      "File \u001b[0;32m~/mambaforge/envs/slickformer/lib/python3.9/site-packages/torch/utils/data/datapipes/_hook_iterator.py:173\u001b[0m, in \u001b[0;36mhook_iterator.<locals>.wrap_generator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    171\u001b[0m         response \u001b[39m=\u001b[39m gen\u001b[39m.\u001b[39msend(\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    172\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 173\u001b[0m     response \u001b[39m=\u001b[39m gen\u001b[39m.\u001b[39;49msend(\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    175\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    176\u001b[0m     datapipe\u001b[39m.\u001b[39m_number_of_samples_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/slickformer/ceruleanml/data_pipeline.py:319\u001b[0m, in \u001b[0;36mMask2FormerProcessorDP.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    317\u001b[0m     sample_dict[\u001b[39m'\u001b[39m\u001b[39mmasks\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m sample_dict[\u001b[39m'\u001b[39m\u001b[39mmasks\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m    318\u001b[0m     sample_dict[\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m sample_dict[\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[0;32m--> 319\u001b[0m instance_mask, instance_id_to_semantic_id \u001b[39m=\u001b[39m masks_to_instance_mask_and_dict(sample_dict[\u001b[39m'\u001b[39;49m\u001b[39mmasks\u001b[39;49m\u001b[39m'\u001b[39;49m], sample_dict[\u001b[39m'\u001b[39;49m\u001b[39mlabels\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    320\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(np\u001b[39m.\u001b[39munique(instance_mask)) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    321\u001b[0m inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessor(images\u001b[39m=\u001b[39m[sample_dict[\u001b[39m'\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m'\u001b[39m]], segmentation_maps\u001b[39m=\u001b[39m[instance_mask], instance_id_to_semantic_id\u001b[39m=\u001b[39m instance_id_to_semantic_id, task_inputs\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mpanoptic\u001b[39m\u001b[39m\"\u001b[39m], return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/slickformer/ceruleanml/data_pipeline.py:289\u001b[0m, in \u001b[0;36mmasks_to_instance_mask_and_dict\u001b[0;34m(mask_arrays, class_ids, starting_instance_id)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmasks_to_instance_mask_and_dict\u001b[39m(mask_arrays, class_ids, starting_instance_id\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m):\n\u001b[1;32m    278\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Instead of squashing instance masks, combines instance masks into an instance mask \u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[39m    and return it plus dict mapping instance ids to class ids.\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39m        _type_: _description_\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m     instance_mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mzeros_like(mask_arrays[\u001b[39m0\u001b[39;49m], dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mint32)\n\u001b[1;32m    290\u001b[0m     instance_id_to_semantic_id \u001b[39m=\u001b[39m {}\n\u001b[1;32m    292\u001b[0m     \u001b[39mfor\u001b[39;00m i, (mask, class_id) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mzip\u001b[39m(mask_arrays, class_ids), start\u001b[39m=\u001b[39mstarting_instance_id):\n",
      "\u001b[0;31mTypeError\u001b[0m: zeros_like(): argument 'input' (position 1) must be Tensor, not numpy.ndarray\nThis exception is thrown by __iter__ of Mask2FormerProcessorDP(kwargs={}, sample_dicts=ChannelFirstNormToTensor)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for i in train_dp_batched:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "91216f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.81 s, sys: 58.4 ms, total: 2.86 s\n",
      "Wall time: 2.91 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "x = next(iter(train_labels_pipe_processed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b465690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'masks': [array([[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)],\n",
       " 'labels': [3],\n",
       " 'image_name': 'S1B_IW_GRDH_1SDV_20210723T164019_20210723T164044_027924_0354FB_5BDF.tif'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9854fe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tifffile import imwrite\n",
    "masks_dir = \"./tiled_masks\"\n",
    "for i in train_labels_pipe_processed:\n",
    "    save_masks_to_tiff(i['masks'], masks_dir, i['image_name'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "562398b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save the masks\n",
    "def save_masks_to_tiff(masks, masks_dir, image_name):\n",
    "    mask_stack = np.stack(masks, axis=-1)  # stack along the channel axis\n",
    "    mask_path = os.path.join(masks_dir, f\"{image_name[-4:]}_mask.tif\")\n",
    "    imwrite(mask_path, mask_stack)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bec90ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pth = 'S1A_IW_GRDH_1SDV_20200729T021511_20200729T021536_033663_03E6CA_2CFE.tif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "279c60d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.tif'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pth[-4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caed843",
   "metadata": {},
   "outputs": [],
   "source": [
    "onef_dm.setup(stage=\"train\") #what's the purpose of stage?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2782ea53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MaxNLocator, FuncFormatter\n",
    "\n",
    "onef_dm.show_batch(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2f5021",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in onef_dm.train_dataloader():\n",
    "    i\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e61ebc4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "class Backbone(nn.Module):\n",
    "    def __init__(\n",
    "        self, model_name, in_chans, num_classes, pretrained, global_pool, drop_rate\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # loads from huggingface if not downloaded\n",
    "        \n",
    "        #by default the above method sets eval mode, set to training\n",
    "        self.backbone.train()\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return self.backbone(xb)\n",
    "\n",
    "class Mask2FormerLightningModel(L.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_path,\n",
    "        learning_rate: float = 2e-5,\n",
    "        adam_epsilon: float = 1e-8,\n",
    "        warmup_steps: int = 0,\n",
    "        weight_decay: float = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()  # saves all hparams as self.hparams\n",
    "        # can try other universal segmentation models: https://github.com/huggingface/transformers/pull/20766/files#r1050493186\n",
    "        config = Mask2FormerConfig.from_pretrained(config_path, ignore_mismatched_sizes=True) \n",
    "        self.model = Mask2FormerForUniversalSegmentation.from_pretrained(\"facebook/mask2former-swin-small-cityscapes-panoptic\", config=config, cache_dir=\"./model_cache\", ignore_mismatched_sizes=True)\n",
    "        #by default the above method sets eval mode, set to training\n",
    "        self.model.train()\n",
    "        # Move the model to the default CUDA device (if available)\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(device)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return self.model(xb)\n",
    "\n",
    "    def one_step(self, batch):\n",
    "        # potential edge case, we squash instance masks to semantic masks. it's possible to lose \n",
    "        # mask label 3 from mask but not class labels since we don't edit class labels\n",
    "        #self.model.train() # for some reason this needs to be set here, not picked up in init\n",
    "        outputs = self.model(pixel_values=batch[\"pixel_values\"], mask_labels=batch[\"mask_labels\"], class_labels=batch['class_labels'])\n",
    "        return outputs.loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Prepare optimizer and schedule (linear warmup and decay)\"\"\"\n",
    "        optimizer = AdamW(self.model.parameters(), lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n",
    "        return [optimizer]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self.one_step(batch)\n",
    "        self.log(\"trn_loss\", loss, on_step=True, on_epoch=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self.one_step(batch)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, score = self.one_step(batch)\n",
    "        self.log(\"tst_loss\", loss, prog_bar=True, logger=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f3f987",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.profilers import AdvancedProfile, SimpleProfiler\n",
    "\n",
    "profiler = SimpleProfiler(dirpath=\".\", filename=\"perf_logs\")\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs = 5,\n",
    "    accelerator=\"auto\",\n",
    "    devices = 1 if torch.cuda.is_available else None,\n",
    "    logger=logger,\n",
    "    profiler=profiler\n",
    "    # deterministic=True # can't set this when using cuda\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c3d5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config_path= \"/home/work/slickformer/custom_models/config.json\"\n",
    "model = Mask2FormerLightningModel(model_config_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "73d9e2d2",
   "metadata": {},
   "source": [
    "need to run this in terminal because for some reason created dirs are owned by root even though docker container built for user 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002d4faa",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "trainer.fit(model, datamodule=onef_dm)\n",
    "print(profiler.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aaad40a",
   "metadata": {
    "lines_to_next_cell": 3
   },
   "outputs": [],
   "source": [
    "# from transformers import  AutoImageProcessor, MaskFormerForInstanceSegmentation\n",
    "# image_processor = AutoImageProcessor.from_pretrained(\"facebook/maskformer-swin-base-coco\", num_labels=3)\n",
    "# instance_inputs = image_processor(images=data['image'], return_tensors=\"pt\")\n",
    "# for k,v in instance_inputs.items():\n",
    "#   print(k,v.shape)\n",
    "\n",
    "\n",
    "# from collections import defaultdict\n",
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib import cm\n",
    "# import matplotlib.patches as mpatches\n",
    "\n",
    "# def draw_panoptic_segmentation(segmentation, segments_info):\n",
    "#     # get the used color map\n",
    "#     viridis = cm.get_cmap('viridis', torch.max(segmentation))\n",
    "#     fig, ax = plt.subplots()\n",
    "#     ax.imshow(segmentation)\n",
    "#     instances_counter = defaultdict(int)\n",
    "#     handles = []\n",
    "#     # for each segment, draw its legend\n",
    "#     for segment in segments_info:\n",
    "#         segment_id = segment['id']\n",
    "#         segment_label_id = segment['label_id']\n",
    "#         segment_label = model.config.id2label[segment_label_id]\n",
    "#         label = f\"{segment_label}-{instances_counter[segment_label_id]}\"\n",
    "#         instances_counter[segment_label_id] += 1\n",
    "#         color = viridis(segment_id)\n",
    "#         handles.append(mpatches.Patch(color=color, label=label))\n",
    "        \n",
    "#     ax.legend(handles=handles)\n",
    "#     plt.savefig('cats_panoptic.png')\n",
    "\n",
    "# draw_panoptic_segmentation(**panoptic_segmentation)\n",
    "\n",
    "# # %%\n",
    "\n",
    "\n",
    "# # %%\n",
    "\n",
    "\n",
    "# # %%\n",
    "\n",
    "\n",
    "# # %%\n",
    "\n",
    "\n",
    "# # %%\n",
    "\n",
    "\n",
    "# # %% [markdown]\n",
    "# # Groundtruth datapipe with non cropped images. We will use these for inference with the trained model.\n",
    "\n",
    "# # %%\n",
    "# gt_train_dp = (train_dp\n",
    "#                     .map(evaluation.remap_gt_dict)\n",
    "#                     .map(evaluation.stack_boxes)\n",
    "# )\n",
    "\n",
    "# # %%\n",
    "# from torchmetrics import detection\n",
    "\n",
    "# m = detection.mean_ap.MeanAveragePrecision(box_format='xyxy', iou_type='bbox', iou_thresholds=[.5], rec_thresholds=None, max_detection_thresholds=None, class_metrics=True)\n",
    "\n",
    "# m.update(preds=[pred_dict_thresholded_nms], target=[test_sample])\n",
    "\n",
    "# from pprint import pprint\n",
    "# pprint(m.compute())"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python [conda env:slickformer]",
   "language": "python",
   "name": "conda-env-slickformer-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
